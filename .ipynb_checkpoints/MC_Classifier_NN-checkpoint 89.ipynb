{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencial NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from config.config import *\n",
    "from config.constants import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score,multilabel_confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(hist):\n",
    "    fig, axs = plt.subplots(nrows=1, figsize=(11, 9))\n",
    "    plt.rcParams['font.size'] = '14'\n",
    "\n",
    "    for label in (axs.get_xticklabels() + axs.get_yticklabels()):\n",
    "        label.set_fontsize(14)    \n",
    "\n",
    "    plt.plot(hist.history['accuracy'])\n",
    "    plt.plot(hist.history['val_accuracy'])\n",
    "\n",
    "    axs.set_title('Model Accuracy')\n",
    "    axs.set_ylabel('Accuracy', fontsize=14)\n",
    "    axs.set_xlabel('Epoch', fontsize=14)\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Model has training accuracy of {:.2f}%\".format(hist.history['accuracy'][-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_split(path):\n",
    "    \n",
    "    dataset = pd.read_csv(path)\n",
    "    dataset.dropna(inplace = True)\n",
    "    \n",
    "    # assigning new column names to the dataframe\n",
    "    # dataset.columns = constants.cols + ['label']\n",
    "    \n",
    "    # creating training set ignoring labels\n",
    "    train_data = dataset[dataset.columns[:-1]].values\n",
    "    scaler = StandardScaler()    \n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "\n",
    "    labels = dataset['label'].values\n",
    "    n_class = len(set(labels))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test =   train_test_split(train_data, labels, test_size=0.20)\n",
    "\n",
    "    X_train = X_train.reshape(-1, 1, train_data.shape[1])\n",
    "    X_test  = X_test.reshape(-1, 1, train_data.shape[1])\n",
    "    y_train = y_train.reshape(-1, 1, 1)\n",
    "    y_test = y_test.reshape(-1, 1, 1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot = None\n",
    "def model_config_train(name,eps,bs,actvn,datalink):\n",
    "    global box_plot\n",
    "    print(\"processing dataset\")\n",
    "    X_train, X_test, y_train, y_test, n_class = pre_process_split(datalink)\n",
    "    \n",
    "    print(n_class)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(n_class, activation=actvn))\n",
    "    print(model.summary())\n",
    "       \n",
    "    chk = ModelCheckpoint(name+'.pkl',save_best_only=True, mode='auto', verbose=1)\n",
    "    print(\"saving as:\",name+'.pkl')\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    hist = model.fit(X_train, y_train, epochs=eps, batch_size=bs, callbacks=[chk], validation_split=0.2)    \n",
    "    plot_model(hist)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset for binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(plot_data,unique_labels,n_plots):\n",
    "    \n",
    "    data = plot_data.copy()\n",
    "    predicted_labels = data['label']\n",
    "    #print(len(set(predicted_labels)),unique_labels)\n",
    "    #print(Counter(predicted_labels).values(),[unique_labels[each] for each in Counter(predicted_labels).keys()])\n",
    "    \n",
    "    matrics = sorted(zip([unique_labels[each] for each in Counter(predicted_labels).keys()],Counter(predicted_labels).values() ), key=lambda x: x[1])\n",
    "    cols = ['A'+str(each+1) for each in range(int(col_len/2))] + ['V'+str(each+1) for each in range(int(col_len/2))]\n",
    "    score = [list(j) for j in matrics][::-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    total = sum([i[1] for i in score])\n",
    "\n",
    "    c=0\n",
    "    for i in score:\n",
    "\n",
    "        score[c][1] = str(round(i[1]*100/total,2))+\"%\"\n",
    "        #print(\"Fault type:\", i[-1], \"Percentage: {:.2f}%\".format(i[1]*100/total))\n",
    "        c+=1\n",
    "\n",
    "    print(pd.DataFrame.from_records(score,columns=['Fault type','Percentage']))\n",
    "    \n",
    "    #print(\"changing numbers to labels again\")\n",
    "    data['label'] = [unique_labels[i] for i in predicted_labels]\n",
    "\n",
    "    fig, ax = plt.subplots(n_plots,figsize=(15,4*n_plots))\n",
    "\n",
    "    for j in range(n_plots):\n",
    "\n",
    "        legend_list = []\n",
    "        for i in range(len(set(predicted_labels))):\n",
    "\n",
    "            extract = data[data.label==unique_labels[i]][cols[j]]    \n",
    "\n",
    "            #print(len(extract))\n",
    "            if unique_labels[i]==score[0][0] and score[0][0]!='NML' or unique_labels[i]== 'FAULT':\n",
    "                temp = ax[j].scatter(extract.index,extract,marker='+',s=40)\n",
    "            else:\n",
    "                temp = ax[j].scatter(extract.index,extract,marker='.',s=10)\n",
    "\n",
    "\n",
    "            legend_list.append(temp)\n",
    "\n",
    "        ax[j].legend(legend_list,unique_labels,scatterpoints=3,ncol=1,fontsize=15)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(model,frame):\n",
    "    \n",
    "    data = frame\n",
    "    \n",
    "    cols = ['A'+str(each+1) for each in range(int(col_len/2))] + ['V'+str(each+1) for each in range(int(col_len/2))]\n",
    "    \n",
    "    if data.shape[1]==6:\n",
    "        data.columns = cols\n",
    "    elif data.shape[1]==7:\n",
    "        data.columns = cols + ['label']\n",
    "        data = data[cols]\n",
    "    else:\n",
    "        print(\"columns length is \",data.shape[1])\n",
    "    \n",
    "    \n",
    "    test_preds = model.predict(data.values.reshape(-1,1,6).tolist())\n",
    "    predicted_labels = np.argmax(test_preds,axis=1)\n",
    "    \n",
    "    data['label'] = predicted_labels\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b0d0c1f07bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_config_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'binary_clf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./KMTrainingSet/binary/bin_dataset_simulink.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-74ea3b958360>\u001b[0m in \u001b[0;36mmodel_config_train\u001b[0;34m(name, eps, bs, actvn, datalink)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbox_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processing dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatalink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6dc7081e9afc>\u001b[0m in \u001b[0;36mpre_process_split\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# creating training set ignoring labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "model_config_train('binary_clf',30,2000,'softmax','./KMTrainingSet/binary/bin_dataset_simulink.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_train('multi_clf',50,2000,'softmax','./KMTrainingSet/multi/mul_dataset_simulink.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels_list = ['NML','FAULT']\n",
    "binary_model = load_model('binary_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_labels_list = ['AB', 'AC', 'BC', 'ABC', 'AG', 'BG', 'ABG', 'CG', 'ACG', 'BCG', 'ABCG']\n",
    "multi_model = load_model('multi_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current directory\n",
    "path = \"./TrainingSet/\"\n",
    "\n",
    "# list of file of the given path is assigned to the variable \n",
    "file_list = [each for each in list(os.walk(path))[0][-1] if \".csv\" in each]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_failing_the_test = []\n",
    "files_passing_the_test = []\n",
    "\n",
    "for each in files_failing_the_test:\n",
    "    print(\"\\n.\\n.\\n\",each)  \n",
    "    temp = tester(binary_model,pd.read_csv('./TrainingSet/'+each))\n",
    "    plotter(temp,binary_labels_list,2)\n",
    "    temp = tester(multi_model,temp[temp.label!=0])\n",
    "    high = plotter(temp,multi_labels_list,2)\n",
    "    if high == ''.join([i for i in each.split(\".\")[0] if not i.isdigit()]):\n",
    "        files_passing_the_test.append(each)\n",
    "    else:\n",
    "        files_failing_the_test.append(each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_passing_the_test,\"....\\n\\n....\",files_failing_the_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [''.join([i for i in each.split(\".\")[0] if not i.isdigit()]) for each in files_passing_the_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_failing_the_test = []\n",
    "files_passing_the_test = []\n",
    "for i in range(1,10):\n",
    "    print(\"___________________________________________________________________\")\n",
    "    temp = tester(binary_model,pd.read_csv('./TrainingSet/'+str(i)+'ABC.csv'))\n",
    "    plotter(temp,binary_labels_list,2)\n",
    "    temp = tester(multi_model,temp[temp.label!=0])\n",
    "    high = plotter(temp,multi_labels_list,2)\n",
    "    if high == ''.join([i for i in each.split(\".\")[0] if not i.isdigit()]):\n",
    "        checker.append(str(i)+'ABC.csv')\n",
    "    else:\n",
    "        checker.append('incorrect')\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_failing_model = [str(i)+'ABC.csv' for i in range(len(checker)) if checker[i]=='incorrect']\n",
    "\n",
    "names = [''.join([i for i in each.split(\".\")[0] if not i.isdigit()]) for each in files_failing_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./TrainingSet/1AG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data['3V'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = Counter((round(data['3V'])/10))\n",
    "matrics = sorted(zip([each for each in Counter(dat).keys()],Counter(dat).values() ), key=lambda x: x[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('KMTrainingset/2ABG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[data.columns[:-1]].values.tolist()\n",
    "#true_labels = data['label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=2,\n",
    "    n_init=10,\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.fit_predict(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['label']=labels\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dic[1]>dic[0]:\n",
    "    print(\"1 = 0 , 0 =1\")\n",
    "    data['label']=[1 if i == 0 else 0 for i in labels]\n",
    "else:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Counter(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = 6\n",
    "fig, ax = plt.subplots(n_plots,figsize=(15,4*n_plots))\n",
    "unique_labels = ['NML','Fault']\n",
    "cols = data.columns[:-1]\n",
    "\n",
    "for j in range(6):\n",
    "\n",
    "    \n",
    "    legend_list = []\n",
    "    for i in list(set(data.label)):\n",
    "        \n",
    "        plo = data[data.label == i]\n",
    "        temp = ax[j].scatter(plo.index,plo[cols[j]],marker='+',s=40)  \n",
    "\n",
    "\n",
    "        legend_list.append(temp)\n",
    "\n",
    "    ax[j].legend(legend_list,unique_labels,scatterpoints=3,ncol=1,fontsize=15)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org = [0,1,0,1,1,1,0,0,1,0,1]\n",
    "[1 if i == 0 else 0 for i in org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(org,[1 if i == 0 else 0 for i in org]):\n",
    "    print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
